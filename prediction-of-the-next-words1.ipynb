{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"> **Table Content**\n> \n    Introduction\n    Import libraries and packages\n    Dataset Information\n    Separate 'Title' field and preprocess it\n        Removing unwanted charaters and words\n        Tokenization and word_index (vocabulary)\n        Convert titles into sequences and Make n_gram model\n        Make all titles with same length and padding them\n    Preprare features (X) and labels (Y)\n    Architechture of Bidirectional LSTM neural network\n    Train Bi-LSTM neural network\n    Plotting accuracy and loss graph\n    Predict new title (Testing)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-07-30T08:58:21.639976Z","iopub.execute_input":"2023-07-30T08:58:21.641324Z","iopub.status.idle":"2023-07-30T08:58:25.278824Z","shell.execute_reply.started":"2023-07-30T08:58:21.641277Z","shell.execute_reply":"2023-07-30T08:58:25.277545Z"}}},{"cell_type":"markdown","source":"<a name=\"intro\"></a>\n\n# Introduction\n\n**Next Word Prediction (also called Language Modeling) is the task of predicting what word comes next. It is one of the fundamental tasks of NLP.**\n\nImage reference: https://medium.com/@antonio.lopardo/the-basics-of-language-modeling-1c8832f21079\n\n![gg.png](attachment:426089b0-5844-4928-a797-40e0015c1a93.png)\n\n#### Application Language Modelling \n\n**1) Mobile keyboard text recommandation**\n\n![fff.jpg](attachment:0cd813a1-ea03-40b9-86d7-0585d994a36e.jpg)\n\n**2) Whenever we search for something on any search engine, we get many suggestions and,  as we type new words in it, we get better recommendations according to our searching context. So, how will it happen??? **\n\n![Screenshot (21).png](attachment:72ee772e-4ef9-4e79-a364-5dcf8f558e4a.png)\n\n\nIt is poosible through natural language processing (NLP) technique. Here, we will use NLP and try to make a prediction model using Bidirectional LSTM (Long short-term memory) model that will predict next words of sentence.\n ","metadata":{}},{"cell_type":"markdown","source":"# Application Language Modelling> >   \n\n> **\n\n1) Mobile keyboard text recommandation\n\n\n\n*2) Whenever we search for something on any search engine, we get many suggestions and, as we type new words in it, we get better recommendations according to our searching context. So, how will it happen??? *\n\n\n\nIt is poosible through natural language processing (NLP) technique. Here, we will use NLP and try to make a prediction model using Bidirectional LSTM (Long short-term memory) model that will predict next words of sentence.**","metadata":{}},{"cell_type":"markdown","source":"# Import necessary libraries and packages> ****","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport os\nimport numpy as np\n\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import Adam","metadata":{"execution":{"iopub.status.busy":"2023-07-31T16:32:48.714676Z","iopub.execute_input":"2023-07-31T16:32:48.715073Z","iopub.status.idle":"2023-07-31T16:32:59.069469Z","shell.execute_reply.started":"2023-07-31T16:32:48.715040Z","shell.execute_reply":"2023-07-31T16:32:59.068299Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Dataset information> \n\n**\nDataset information\n\nImport Medium-articles-dataset:\n\nThis dataset contains information about randomly chosen medium articles published in 2019 from these 7 publications:\n\n    Towards Data Science\n    UX Collective\n    The Startup\n    The Writing Cooperative\n    Data Driven Investor\n    Better Humans\n    Better Marketing\n\n**","metadata":{}},{"cell_type":"code","source":"medium_data = pd.read_csv('../input/medium-articles-dataset/medium_data.csv')\nmedium_data.head()","metadata":{"execution":{"iopub.status.busy":"2023-07-31T16:32:59.071333Z","iopub.execute_input":"2023-07-31T16:32:59.072064Z","iopub.status.idle":"2023-07-31T16:32:59.203804Z","shell.execute_reply.started":"2023-07-31T16:32:59.072029Z","shell.execute_reply":"2023-07-31T16:32:59.202998Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"   id                                                url  \\\n0   1  https://towardsdatascience.com/a-beginners-gui...   \n1   2  https://towardsdatascience.com/hands-on-graph-...   \n2   3  https://towardsdatascience.com/how-to-use-ggpl...   \n3   4  https://towardsdatascience.com/databricks-how-...   \n4   5  https://towardsdatascience.com/a-step-by-step-...   \n\n                                               title  \\\n0  A Beginner’s Guide to Word Embedding with Gens...   \n1  Hands-on Graph Neural Networks with PyTorch & ...   \n2                       How to Use ggplot2 in Python   \n3  Databricks: How to Save Files in CSV on Your L...   \n4  A Step-by-Step Implementation of Gradient Desc...   \n\n                                  subtitle   image  claps responses  \\\n0                                      NaN   1.png    850         8   \n1                                      NaN   2.png   1100        11   \n2         A Grammar of Graphics for Python   3.png    767         1   \n3  When I work on Python projects dealing…  4.jpeg    354         0   \n4          One example of building neural…  5.jpeg    211         3   \n\n   reading_time           publication        date  \n0             8  Towards Data Science  2019-05-30  \n1             9  Towards Data Science  2019-05-30  \n2             5  Towards Data Science  2019-05-30  \n3             4  Towards Data Science  2019-05-30  \n4             4  Towards Data Science  2019-05-30  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>url</th>\n      <th>title</th>\n      <th>subtitle</th>\n      <th>image</th>\n      <th>claps</th>\n      <th>responses</th>\n      <th>reading_time</th>\n      <th>publication</th>\n      <th>date</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>https://towardsdatascience.com/a-beginners-gui...</td>\n      <td>A Beginner’s Guide to Word Embedding with Gens...</td>\n      <td>NaN</td>\n      <td>1.png</td>\n      <td>850</td>\n      <td>8</td>\n      <td>8</td>\n      <td>Towards Data Science</td>\n      <td>2019-05-30</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>https://towardsdatascience.com/hands-on-graph-...</td>\n      <td>Hands-on Graph Neural Networks with PyTorch &amp; ...</td>\n      <td>NaN</td>\n      <td>2.png</td>\n      <td>1100</td>\n      <td>11</td>\n      <td>9</td>\n      <td>Towards Data Science</td>\n      <td>2019-05-30</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>https://towardsdatascience.com/how-to-use-ggpl...</td>\n      <td>How to Use ggplot2 in Python</td>\n      <td>A Grammar of Graphics for Python</td>\n      <td>3.png</td>\n      <td>767</td>\n      <td>1</td>\n      <td>5</td>\n      <td>Towards Data Science</td>\n      <td>2019-05-30</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>https://towardsdatascience.com/databricks-how-...</td>\n      <td>Databricks: How to Save Files in CSV on Your L...</td>\n      <td>When I work on Python projects dealing…</td>\n      <td>4.jpeg</td>\n      <td>354</td>\n      <td>0</td>\n      <td>4</td>\n      <td>Towards Data Science</td>\n      <td>2019-05-30</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>https://towardsdatascience.com/a-step-by-step-...</td>\n      <td>A Step-by-Step Implementation of Gradient Desc...</td>\n      <td>One example of building neural…</td>\n      <td>5.jpeg</td>\n      <td>211</td>\n      <td>3</td>\n      <td>4</td>\n      <td>Towards Data Science</td>\n      <td>2019-05-30</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"print(\"Number of records: \", medium_data.shape[0])\nprint(\"Number of fields: \", medium_data.shape[1])","metadata":{"execution":{"iopub.status.busy":"2023-07-31T16:32:59.205018Z","iopub.execute_input":"2023-07-31T16:32:59.205520Z","iopub.status.idle":"2023-07-31T16:32:59.211221Z","shell.execute_reply.started":"2023-07-31T16:32:59.205490Z","shell.execute_reply":"2023-07-31T16:32:59.209896Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Number of records:  6508\nNumber of fields:  10\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Here, we have a 10 different fields and 6508 records but we will only use title field for predicting next word.\n","metadata":{}},{"cell_type":"markdown","source":"# > Display titles of various articles and preprocess them¶","metadata":{}},{"cell_type":"code","source":"medium_data['title']","metadata":{"execution":{"iopub.status.busy":"2023-07-31T16:32:59.213834Z","iopub.execute_input":"2023-07-31T16:32:59.214747Z","iopub.status.idle":"2023-07-31T16:32:59.231162Z","shell.execute_reply.started":"2023-07-31T16:32:59.214699Z","shell.execute_reply":"2023-07-31T16:32:59.230108Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"0       A Beginner’s Guide to Word Embedding with Gens...\n1       Hands-on Graph Neural Networks with PyTorch & ...\n2                            How to Use ggplot2 in Python\n3       Databricks: How to Save Files in CSV on Your L...\n4       A Step-by-Step Implementation of Gradient Desc...\n                              ...                        \n6503    “We” vs “I” — How Should You Talk About Yourse...\n6504                     How Donald Trump Markets Himself\n6505        Content and Marketing Beyond Mass Consumption\n6506    5 Questions All Copywriters Should Ask Clients...\n6507               How To Write a Good Business Blog Post\nName: title, Length: 6508, dtype: object"},"metadata":{}}]},{"cell_type":"markdown","source":"# Removing unwanted characters and words in titles>  \n\nLooking at titles, we can see there are some of unwanted characters and words in it which can not be useful for us to predict infact it might decrease our model accuracy so we have to remove it.","metadata":{}},{"cell_type":"code","source":"medium_data['title'] = medium_data['title'].apply(lambda x: x.replace(u'\\xa0',u' '))\nmedium_data['title'] = medium_data['title'].apply(lambda x: x.replace('\\u200a',' '))","metadata":{"execution":{"iopub.status.busy":"2023-07-31T16:32:59.232822Z","iopub.execute_input":"2023-07-31T16:32:59.233724Z","iopub.status.idle":"2023-07-31T16:32:59.251173Z","shell.execute_reply.started":"2023-07-31T16:32:59.233680Z","shell.execute_reply":"2023-07-31T16:32:59.250222Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# Tokenzation> \n\nTokenzaion is the process in which we provide an unique id to all the words and make a word index or we can say vocabulary.","metadata":{}},{"cell_type":"code","source":"tokenizer = Tokenizer(oov_token='<oov>') # For those words which are not found in word_index\ntokenizer.fit_on_texts(medium_data['title'])\ntotal_words = len(tokenizer.word_index) + 1\n\nprint(\"Total number of words: \", total_words)\nprint(\"Word: ID\")\nprint(\"------------\")\nprint(\"<oov>: \", tokenizer.word_index['<oov>'])\nprint(\"Strong: \", tokenizer.word_index['strong'])\nprint(\"And: \", tokenizer.word_index['and'])\nprint(\"Consumption: \", tokenizer.word_index['consumption'])","metadata":{"execution":{"iopub.status.busy":"2023-07-31T16:32:59.252695Z","iopub.execute_input":"2023-07-31T16:32:59.253471Z","iopub.status.idle":"2023-07-31T16:32:59.395118Z","shell.execute_reply.started":"2023-07-31T16:32:59.253436Z","shell.execute_reply":"2023-07-31T16:32:59.394285Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Total number of words:  8238\nWord: ID\n------------\n<oov>:  1\nStrong:  4\nAnd:  8\nConsumption:  8237\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<a name=\"ngram\"></a>\n#### Titles text into sequences and make n_gram model\n\nsuppose, we have sentence like **\"I am Yash\"** and this will convert into a sequence with their respective tokens **{'I': 1,'am': 2,'Yash': 3}**. Thus, output will be  **[ '1' ,'2' ,'3' ]**\n\nLikewise, our all titles will be converted into sequences.\n\nThen,\nwe will make a n_gram model for good prediction.\n\nBelow image explain about everything.\n\n![Capture.PNG](attachment:48ad80b3-90bf-4cf6-99f8-7dcfd467d1f8.PNG)\n","metadata":{}},{"cell_type":"code","source":"input_sequences = []\nfor line in medium_data['title']:\n    token_list = tokenizer.texts_to_sequences([line])[0]\n    #print(token_list)\n    \n    for i in range(1, len(token_list)):\n        n_gram_sequence = token_list[:i+1]\n        input_sequences.append(n_gram_sequence)\n\n#print(input_sequences)\nprint(\"Total input sequences: \", len(input_sequences))","metadata":{"execution":{"iopub.status.busy":"2023-07-31T16:32:59.396211Z","iopub.execute_input":"2023-07-31T16:32:59.396932Z","iopub.status.idle":"2023-07-31T16:32:59.532342Z","shell.execute_reply.started":"2023-07-31T16:32:59.396880Z","shell.execute_reply":"2023-07-31T16:32:59.531121Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Total input sequences:  48461\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Make all titles with same length by using padding> \n\nThe length of every title has to be the same. To make it, we need to find a title that has a maximum length, and based on that length, we have to pad rest of titles.","metadata":{}},{"cell_type":"code","source":"# pad sequences \nmax_sequence_len = max([len(x) for x in input_sequences])\ninput_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\ninput_sequences[1]","metadata":{"execution":{"iopub.status.busy":"2023-07-31T16:32:59.533927Z","iopub.execute_input":"2023-07-31T16:32:59.534442Z","iopub.status.idle":"2023-07-31T16:32:59.685038Z","shell.execute_reply.started":"2023-07-31T16:32:59.534400Z","shell.execute_reply":"2023-07-31T16:32:59.683779Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   5, 676,\n        68], dtype=int32)"},"metadata":{}}]},{"cell_type":"markdown","source":"# Prepare features and labels> \n\nHere, we consider last element of all sequences as a label.Then, We need to perform onehot encoding on labels corresponding to total_words.","metadata":{}},{"cell_type":"code","source":"# create features and label\nxs, labels = input_sequences[:,:-1],input_sequences[:,-1]\nys = tf.keras.utils.to_categorical(labels, num_classes=total_words)","metadata":{"execution":{"iopub.status.busy":"2023-07-31T16:32:59.686277Z","iopub.execute_input":"2023-07-31T16:32:59.686628Z","iopub.status.idle":"2023-07-31T16:32:59.852661Z","shell.execute_reply.started":"2023-07-31T16:32:59.686591Z","shell.execute_reply":"2023-07-31T16:32:59.851464Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"print(xs[5])\nprint(labels[5])\nprint(ys[5][14])","metadata":{"execution":{"iopub.status.busy":"2023-07-31T16:32:59.855851Z","iopub.execute_input":"2023-07-31T16:32:59.856243Z","iopub.status.idle":"2023-07-31T16:32:59.862749Z","shell.execute_reply.started":"2023-07-31T16:32:59.856209Z","shell.execute_reply":"2023-07-31T16:32:59.861612Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    5  676   68    2  452 1518]\n14\n1.0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<a name=\"blstm\"></a>\n# Architechture of Bidirectional LSTM Neural Network\n\nLong Short-Term Memory (LSTM) networks is an advance recurrent neural network which is apable to store order states by using its cell state feature.\n\nImage reference: https://www.researchgate.net/figure/The-structure-of-the-Long-Short-Term-Memory-LSTM-neural-network-Reproduced-from-Yan_fig8_334268507\n![lstm.png](attachment:c34341f6-d243-478a-b4bd-bf242759cd50.png)\n\n**Bidirectional LSTM**\nImage reference: https://paperswithcode.com/method/bilstm\n![bi.png](attachment:d26c6b0c-cbdf-45a5-b88b-2b352d7b7d63.png)","metadata":{}},{"cell_type":"markdown","source":"# > Bi- LSTM Neural Network Model training\n","metadata":{}},{"cell_type":"code","source":"model = Sequential()\nmodel.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\nmodel.add(Bidirectional(LSTM(150)))\nmodel.add(Dense(total_words, activation='softmax'))\nadam = Adam(lr=0.01)\nmodel.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\nhistory = model.fit(xs, ys, epochs=50, verbose=1)\n#print model.summary()\nprint(model)\n","metadata":{"execution":{"iopub.status.busy":"2023-07-31T16:32:59.864794Z","iopub.execute_input":"2023-07-31T16:32:59.865392Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1/50\n1515/1515 [==============================] - 152s 98ms/step - loss: 7.0087 - accuracy: 0.0828\nEpoch 2/50\n1515/1515 [==============================] - 147s 97ms/step - loss: 6.1808 - accuracy: 0.1364\nEpoch 3/50\n1515/1515 [==============================] - 137s 91ms/step - loss: 5.6644 - accuracy: 0.1680\nEpoch 4/50\n1515/1515 [==============================] - 135s 89ms/step - loss: 5.1950 - accuracy: 0.1935\nEpoch 5/50\n1515/1515 [==============================] - 137s 90ms/step - loss: 4.7480 - accuracy: 0.2186\nEpoch 6/50\n1515/1515 [==============================] - 141s 93ms/step - loss: 4.3259 - accuracy: 0.2471\nEpoch 7/50\n1515/1515 [==============================] - 147s 97ms/step - loss: 3.9258 - accuracy: 0.2873\nEpoch 8/50\n1515/1515 [==============================] - 147s 97ms/step - loss: 3.5454 - accuracy: 0.3339\nEpoch 9/50\n1515/1515 [==============================] - 146s 96ms/step - loss: 3.1902 - accuracy: 0.3881\nEpoch 10/50\n1515/1515 [==============================] - 138s 91ms/step - loss: 2.8713 - accuracy: 0.4446\nEpoch 11/50\n1515/1515 [==============================] - 137s 91ms/step - loss: 2.5845 - accuracy: 0.4938\nEpoch 12/50\n1515/1515 [==============================] - 137s 91ms/step - loss: 2.3355 - accuracy: 0.5383\nEpoch 13/50\n1515/1515 [==============================] - 137s 90ms/step - loss: 2.1141 - accuracy: 0.5797\nEpoch 14/50\n1515/1515 [==============================] - 136s 90ms/step - loss: 1.9178 - accuracy: 0.6164\nEpoch 15/50\n1515/1515 [==============================] - 136s 90ms/step - loss: 1.7397 - accuracy: 0.6524\nEpoch 16/50\n1515/1515 [==============================] - 136s 89ms/step - loss: 1.5874 - accuracy: 0.6818\nEpoch 17/50\n1515/1515 [==============================] - 136s 90ms/step - loss: 1.4514 - accuracy: 0.7103\nEpoch 18/50\n1515/1515 [==============================] - 136s 90ms/step - loss: 1.3325 - accuracy: 0.7320\nEpoch 19/50\n1515/1515 [==============================] - 135s 89ms/step - loss: 1.2262 - accuracy: 0.7537\nEpoch 20/50\n1515/1515 [==============================] - 136s 90ms/step - loss: 1.1360 - accuracy: 0.7731\nEpoch 21/50\n1515/1515 [==============================] - 136s 90ms/step - loss: 1.0588 - accuracy: 0.7860\nEpoch 22/50\n1515/1515 [==============================] - 135s 89ms/step - loss: 0.9892 - accuracy: 0.7983\nEpoch 23/50\n1515/1515 [==============================] - 136s 90ms/step - loss: 0.9319 - accuracy: 0.8087\nEpoch 24/50\n1515/1515 [==============================] - 134s 89ms/step - loss: 0.8789 - accuracy: 0.8187\nEpoch 25/50\n1515/1515 [==============================] - 136s 90ms/step - loss: 0.8357 - accuracy: 0.8260\nEpoch 26/50\n1333/1515 [=========================>....] - ETA: 16s - loss: 0.7857 - accuracy: 0.8346","output_type":"stream"}]},{"cell_type":"markdown","source":"# **Plotting model accuracy and loss**> ","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n\ndef plot_graphs(history, string):\n    plt.plot(history.history[string])\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(string)\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_graphs(history, 'accuracy')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_graphs(history, 'loss')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Predicting next word of title**> ","metadata":{}},{"cell_type":"code","source":"seed_text = \"implementation of\"\nnext_words = 2\n  \nfor _ in range(next_words):\n    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n    predicted = model.predict_classes(token_list, verbose=0)\n    output_word = \"\"\n    for word, index in tokenizer.word_index.items():\n        if index == predicted:\n            output_word = word\n            break\n    seed_text += \" \" + output_word\nprint(seed_text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}